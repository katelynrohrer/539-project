{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2504c002d7df245",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T21:49:06.873879Z",
     "start_time": "2025-05-06T21:49:06.743447Z"
    }
   },
   "outputs": [],
   "source": [
    "NAME = \"Katelyn Rohrer\"\n",
    "EMAIL = \"katelynrohrer@arizona.edu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc9c2b16ae16e23",
   "metadata": {},
   "source": [
    "Including all necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbc2b6319d5ef76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T21:49:07.703210Z",
     "start_time": "2025-05-06T21:49:06.750985Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Iterator, Iterable, List, Tuple, Text, Union\n",
    "\n",
    "from scipy.sparse import spmatrix\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "# from sklearn.feature_extraction import text\n",
    "\n",
    "NDArray = Union[np.ndarray, spmatrix]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0a230c35186216",
   "metadata": {},
   "source": [
    "#### I started by using the classes defined in our previous assignments: TextToFeatures, TextToLabels, and Classifier.\n",
    "These models were helpful in defining the baseline classification system. I did, however, make some modifications to the exact models used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8490e11524dea77",
   "metadata": {},
   "source": [
    "For my TextToFeatures class, I found that TF-IDF worked the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "567fe86bcdf83e9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T21:49:07.707183Z",
     "start_time": "2025-05-06T21:49:07.704439Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextToFeatures:\n",
    "    \"\"\"\n",
    "    Converts raw text into tf-idf vectors.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Creates the vectorizer.\n",
    "        Word-level preprocessing where any URLs are stripped from tokenization.\n",
    "        Bigram and unigram processing is used, and stop words are not dropped.\n",
    "        \"\"\"\n",
    "        self.vect = TfidfVectorizer(lowercase=True,\n",
    "                                    preprocessor=self.strip_urls,\n",
    "                                    stop_words=None,\n",
    "                                    ngram_range=(1, 2),\n",
    "                                    analyzer=\"word\",\n",
    "                                    binary=False)\n",
    "\n",
    "    def strip_urls(self, training_texts: str):\n",
    "        \"\"\"\n",
    "        Helper method to remove URLs from text.\n",
    "        :return: Str text without URLs\n",
    "        \"\"\"\n",
    "        return re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", training_texts, flags=re.IGNORECASE)\n",
    "\n",
    "    def fit(self, training_texts: Iterable[Text]) -> None:\n",
    "        \"\"\"\n",
    "        Fits the training data using tf-idf\n",
    "        \"\"\"\n",
    "        self.vect.fit(training_texts)\n",
    "\n",
    "    def transform(self, texts: Iterable[Text]) -> NDArray:\n",
    "        \"\"\"\n",
    "        Transforms the text into vectors\n",
    "        :return: NDArray of vectors from the text\n",
    "        \"\"\"\n",
    "        return self.vect.transform(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1290b82ebc740",
   "metadata": {},
   "source": [
    "Then, we need to define IDs for our labels so that our model\n",
    "can assign our vectors to them.\n",
    "I made no significant changes to this class relative to my previous assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "738c30e45ba9cbf4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T21:49:07.710452Z",
     "start_time": "2025-05-06T21:49:07.707733Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextToLabels:\n",
    "    \"\"\"\n",
    "    Converts the sentiment labels into integer values for labelling. \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Defines the LabelEncoder object.\n",
    "        \"\"\"\n",
    "        self.encoder = LabelEncoder()\n",
    "\n",
    "    def fit(self, training_labels: Iterable[Text]) -> None:\n",
    "        \"\"\"\n",
    "        Learns the unique labels and assigns them IDs\n",
    "        \"\"\"\n",
    "        self.encoder.fit(training_labels)\n",
    "\n",
    "    def transform(self, labels: Iterable[Text]) -> NDArray:\n",
    "        \"\"\"\n",
    "        Converts each label into their corresponding ID.\n",
    "        :return: The ID labels as integers\n",
    "        \"\"\"\n",
    "        return self.encoder.transform(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733f1054ad43f967",
   "metadata": {},
   "source": [
    "Lastly, we need to define a classifier that can classify new\n",
    "text as one of the labels.\n",
    "Here, we use logistic regression for our analysis, as that tends to work best with tf-idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad5180c1aa4dfadf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T21:49:07.713603Z",
     "start_time": "2025-05-06T21:49:07.710899Z"
    }
   },
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    \"\"\"\n",
    "    Defines a logistic regression classifier that can read in\n",
    "    new text and classify the sentiment as positive, neutral,\n",
    "    or negative\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Creates the classifier. Sets a threshold for the max\n",
    "        number of iterations (for efficiency) and smooths some\n",
    "        of the data.\n",
    "        \"\"\"\n",
    "        self.classifier = LogisticRegression(max_iter=1000, class_weight=\"balanced\", C=0.5)\n",
    "\n",
    "    def train(self, features: NDArray, labels: NDArray) -> None:\n",
    "        \"\"\"\n",
    "        Trains the classifier on our features and labels to be able to classify text\n",
    "        \"\"\"\n",
    "        self.classifier.fit(features, labels)\n",
    "\n",
    "    def predict(self, features: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        Applies the model to predict the label of new text\n",
    "        :return: Predicted labels for each of the features given\n",
    "        \"\"\"\n",
    "        return self.classifier.predict(features)\n",
    "\n",
    "    def translate(self, prediction: NDArray, labels: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Translates the numerical labels into their corresponding str values\n",
    "        :return: The ID labels as strings\n",
    "        \"\"\"\n",
    "        return [labels[idx] for idx in prediction]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4b098cd86cce0d",
   "metadata": {},
   "source": [
    "Lastly, we should also define how we are going to score our model.\n",
    "For simplicity, I created a wrapper function, but the baseline analysis comes from an import\n",
    "from sklearn which provides precision, recall, and f1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44f71f287772759f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T21:49:07.716736Z",
     "start_time": "2025-05-06T21:49:07.713165Z"
    }
   },
   "outputs": [],
   "source": [
    "def score(df, predictions):\n",
    "    print(\"Train\" if df.columns.all() == train_df.columns.all() else \"Test\")  # this isn't the most graceful, but it works to auto-label the output\n",
    "    print(classification_report(df[\"sentiment\"], predictions, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1f60fff723b17d",
   "metadata": {},
   "source": [
    "Now, we can start using the classes defined to train our classifier and label our data.\n",
    "We start by reading in our training data, traing the classifier, then use the classifier on that data.\n",
    "\n",
    "Note that we used a pandas dataframe here. Personally, I have a lot of experience with pandas,\n",
    "and I found this to be a perfect use-case for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b066b50d22bb5213",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T21:49:07.716945Z",
     "start_time": "2025-05-06T21:49:07.714988Z"
    }
   },
   "outputs": [],
   "source": [
    "train_path = os.path.join(\"data\", \"train.csv\")\n",
    "test_path = os.path.join(\"data\", \"test.csv\")\n",
    "sentiments = [\"negative\", \"neutral\", \"positive\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16f5f94914a79ebc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T21:49:12.220318Z",
     "start_time": "2025-05-06T21:49:07.719043Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8860    0.8987    0.8923      7781\n",
      "     neutral     0.8570    0.8811    0.8689     11118\n",
      "    positive     0.9073    0.8624    0.8843      8582\n",
      "\n",
      "    accuracy                         0.8802     27481\n",
      "   macro avg     0.8834    0.8807    0.8818     27481\n",
      "weighted avg     0.8809    0.8802    0.8803     27481\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(train_path)\n",
    "train_df[\"text\"] = train_df[\"text\"].fillna(\"\")\n",
    "train_df[\"selected_text\"] = train_df[\"selected_text\"].fillna(\"\")\n",
    "\n",
    "# training on the entire text, prioritizing the selected text\n",
    "# (selected text exists both in text and selected_text, so selected text apprears twice)\n",
    "train_df[\"weighted_text\"] = train_df[\"selected_text\"] + \" \" + train_df[\"text\"]\n",
    "\n",
    "model = TextToFeatures()\n",
    "model.fit(train_df[\"weighted_text\"])\n",
    "train_features = model.transform(train_df[\"weighted_text\"])\n",
    "\n",
    "labeler = TextToLabels()\n",
    "labeler.fit(train_df[\"sentiment\"])\n",
    "labels = labeler.transform(train_df[\"sentiment\"])\n",
    "\n",
    "clf = Classifier()\n",
    "clf.train(train_features, labels)\n",
    "\n",
    "train_labels_predicted = clf.translate(clf.predict(train_features), sentiments)\n",
    "\n",
    "score(train_df, train_labels_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6e8049a9100c9d",
   "metadata": {},
   "source": [
    "We can see here that our accuracy is 88%, as well as high 80s for precision and recall, which is a fairly good metric.\n",
    "Since it's not overly inflated, this implies that our model will be robust enough for the testing data.\n",
    "\n",
    "Moving on, now using the same model, we can classify our testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0ed4c759f7e98e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T21:49:12.270244Z",
     "start_time": "2025-05-06T21:49:12.183812Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.6810    0.6653    0.6731      1001\n",
      "     neutral     0.6425    0.7140    0.6764      1430\n",
      "    positive     0.7963    0.6981    0.7440      1103\n",
      "\n",
      "    accuracy                         0.6952      3534\n",
      "   macro avg     0.7066    0.6925    0.6978      3534\n",
      "weighted avg     0.7014    0.6952    0.6965      3534\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(test_path)\n",
    "test_df[\"text\"] = test_df[\"text\"].fillna(\"\")\n",
    "\n",
    "test_features = model.transform(test_df[\"text\"])\n",
    "\n",
    "test_labels_predicted = clf.translate(clf.predict(test_features), sentiments)\n",
    "\n",
    "score(test_df, test_labels_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b82713bcdb9a1a",
   "metadata": {},
   "source": [
    "We can see that our numbers definitely dropped, but they are still relatively high.\n",
    "Accuracy remains just under 70% and precision and recall each have similar metrics.\n",
    "\n",
    "These were the highest stats that I was able to procure. I also attempted changing stop-words,\n",
    "n-gram counts, and changing my TextToFeature and Classifier models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b349e69d521a4899",
   "metadata": {},
   "source": [
    "Now that our testing data is classified, we move on to step 2: determining which phrase contributes most to the output.\n",
    "I started by simply attempting to extract the tokens that contribute most to the output label, without consideration\n",
    "for keeping a singular phrase together. I used this function primarily to get a sense of the data, not for actual output-sake.\n",
    "Since I was just looking at the output manually, I stopped by output after 10 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e19f43f6006ebe9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T21:49:12.957668Z",
     "start_time": "2025-05-06T21:49:12.342467Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral (actual: neutral):\n",
      "\"Last session of the day  http://twitpic.com/67ezh\"\n",
      "  if: weight=0.7401\n",
      "  going: weight=0.5875\n",
      "  have: weight=-0.1857\n",
      "  if were: weight=0.1739\n",
      "  were: weight=0.0829\n",
      "\n",
      "Negative (actual: positive):\n",
      "\" Shanghai is also really exciting (precisely -- skyscrapers galore). Good tweeps in China:  (SH)  (BJ).\"\n",
      "  miss: weight=5.4277\n",
      "  you: weight=-2.0338\n",
      "  miss you: weight=1.0837\n",
      "  will: weight=-0.4848\n",
      "  SAD: weight=0.4845\n",
      "\n",
      "Negative (actual: negative):\n",
      "\"Recession hit Veronique Branquinho, she has to quit her company, such a shame!\"\n",
      "  my: weight=1.8043\n",
      "  me: weight=1.5423\n",
      "  is: weight=0.7767\n",
      "  my boss: weight=0.1849\n",
      "  bullying me: weight=0.1481\n",
      "\n",
      "Negative (actual: positive):\n",
      "\" happy bday!\"\n",
      "  me: weight=1.5423\n",
      "  alone: weight=0.6269\n",
      "  leave: weight=0.5019\n",
      "  what: weight=-0.2786\n",
      "  leave me: weight=0.2318\n",
      "\n",
      "Negative (actual: positive):\n",
      "\" http://twitpic.com/4w75p - I like it!!\"\n",
      "  couldn: weight=0.8179\n",
      "  already: weight=0.7928\n",
      "  we: weight=-0.4450\n",
      "  bought: weight=-0.3572\n",
      "  them: weight=-0.3435\n",
      "\n",
      "Positive (actual: positive):\n",
      "\" that`s great!! weee!! visitors!\"\n",
      "  best: weight=3.4072\n",
      "  the best: weight=1.2546\n",
      "  for: weight=0.9904\n",
      "  on: weight=-0.5389\n",
      "  some: weight=0.3890\n",
      "\n",
      "Positive (actual: negative):\n",
      "\"I THINK EVERYONE HATES ME ON HERE   lol\"\n",
      "  fun: weight=4.3277\n",
      "  for: weight=0.9904\n",
      "  is: weight=-0.4429\n",
      "  baby: weight=0.4324\n",
      "  and: weight=0.4046\n",
      "\n",
      "Neutral (actual: negative):\n",
      "\" soooooo wish i could, but im in school and myspace is completely blocked\"\n",
      "  Soooo high: weight=0.1462\n",
      "  high: weight=-0.1426\n",
      "  Soooo: weight=-0.1040\n",
      "  high Soooo: weight=0.0731\n",
      "\n",
      "Neutral (actual: neutral):\n",
      "\" and within a short time of the last clue all of them\"\n",
      "  Both: weight=0.2171\n",
      "  of you: weight=-0.1789\n",
      "  you: weight=0.1641\n",
      "  Both of: weight=0.1635\n",
      "  of: weight=-0.1099\n",
      "\n",
      "Positive (actual: neutral):\n",
      "\" What did you get?  My day is alright.. haven`t done anything yet. leaving soon to my stepsister though!\"\n",
      "  Wow: weight=1.2656\n",
      "  hehe: weight=0.5364\n",
      "  is: weight=-0.4429\n",
      "  that: weight=-0.3829\n",
      "  just: weight=-0.3353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def explain_prediction(features, clf: Classifier, model: TextToFeatures):\n",
    "    limit = 10\n",
    "    for i, df_row in test_df.iterrows():\n",
    "        feature_names = model.vect.get_feature_names()\n",
    "        coefs = clf.classifier.coef_\n",
    "\n",
    "        row = features[i]\n",
    "        row_indices = row.nonzero()[1]\n",
    "\n",
    "        pred = clf.predict(row)[0]\n",
    "\n",
    "        impacts = [(feature_names[j], coefs[pred][j]) for j in row_indices]\n",
    "        top_impacts = sorted(impacts, key=lambda x: -abs(x[1]))[:5]\n",
    "\n",
    "        print(f\"{sentiments[pred].capitalize()} (actual: {df_row['sentiment']}):\\n\\\"{df_row['text']}\\\"\")\n",
    "        for word, score in top_impacts:\n",
    "            print(f\"  {word}: weight={score:.4f}\")\n",
    "        print()\n",
    "\n",
    "        i += 1\n",
    "        if i == limit:\n",
    "            break\n",
    "            \n",
    "explain_prediction(train_features, clf, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80453f53e0b2a7fa",
   "metadata": {},
   "source": [
    "Once I got a sense of the data, my next attempt was to keep my phrases together.\n",
    "I started by extracting subsets of each tweet of all lengths (1 to length of the tweet) and scoring them on their\n",
    "similarity to the label (previously defined label). Then, it determines which subset of text has the highest contribution\n",
    "towards the label and defines that as the \"selected text\". I aggregate this data into a dataframe and return that. \n",
    "\n",
    "Note that longer phrases are preferred by this function (line 48), as I was initially only getting the highest contributing single token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "699398fb8763c1be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T21:49:12.969914Z",
     "start_time": "2025-05-06T21:49:12.960464Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_top_phrases(df, features, clf: Classifier, model: TextToFeatures):\n",
    "    \"\"\"\n",
    "    Reads in the data, model, and classifier, and uses them to classify each subset of the text.\n",
    "    It then finds the highest scoring subset of text towards the given label and sets that as the\n",
    "    selected text for that given text. This happens for all text entries within the df.\n",
    "    :return: Dataframe containing only the textID and the selected text for that message. \n",
    "    \"\"\"\n",
    "    vocab = model.vect.vocabulary_\n",
    "    coefs = clf.classifier.coef_\n",
    "\n",
    "    results = []\n",
    "    for i, row in df.iterrows():\n",
    "        print(\"\\r\", i, end=\"\")\n",
    "        raw_text = row[\"text\"]\n",
    "        pred = clf.predict(features[i])[0]\n",
    "\n",
    "        # neutral tweets don't have a key sentiment selected text\n",
    "        if sentiments[pred] == \"neutral\":\n",
    "            results.append({\n",
    "                \"textID\": row.get(\"textID\", i),\n",
    "                \"selected_text\": raw_text\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        token_spans = list(re.finditer(r\"\\b\\w+\\b\", raw_text))\n",
    "        tokens = [m.group().lower() for m in token_spans]\n",
    "\n",
    "        best_phrases = []\n",
    "        for n in range(len(tokens)):\n",
    "            for j in range(len(tokens) - n + 1):\n",
    "                span_tokens = tokens[j:j + n]\n",
    "                span = \" \".join(span_tokens)\n",
    "                total_score = 0.0\n",
    "                count = 0\n",
    "\n",
    "                for word in span_tokens:\n",
    "                    idx = vocab.get(word)\n",
    "                    if idx is not None:\n",
    "                        total_score += coefs[pred][idx]\n",
    "                        count += 1\n",
    "\n",
    "                idx = vocab.get(span)\n",
    "                if idx is not None:\n",
    "                    total_score += coefs[pred][idx]\n",
    "                    count += 1\n",
    "\n",
    "                if count > 0:\n",
    "                    avg_score = total_score / count\n",
    "                    weighted_score = avg_score * n  # favor longer spans\n",
    "                    best_phrases.append((j, j + n - 1, weighted_score))\n",
    "\n",
    "        # sort the best phrases and select the top scoring oe as the selected text\n",
    "        if best_phrases:\n",
    "            best_phrases.sort(key=lambda x: -abs(x[2]))\n",
    "            start_i, end_i, _ = best_phrases[0]\n",
    "            start_char = token_spans[start_i].start()\n",
    "            end_char = token_spans[end_i].end()\n",
    "            selected = raw_text[start_char:end_char]\n",
    "        else:\n",
    "            selected = \"\"\n",
    "\n",
    "        results.append({\n",
    "            \"textID\": row.get(\"textID\", i),\n",
    "            \"selected_text\": selected\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9937bb3153dae4",
   "metadata": {},
   "source": [
    "In order to test this function, I used it on the training data and compared my output to the\n",
    "text selected by the training data. It should iterate about 27000 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f754c652ea085fe9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T21:49:48.187129Z",
     "start_time": "2025-05-06T21:49:12.962479Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 27480"
     ]
    }
   ],
   "source": [
    "train_output = extract_top_phrases(train_df, train_features, clf, model)\n",
    "train_output.to_csv(\"train_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1605c14d8d6f3e",
   "metadata": {},
   "source": [
    "Since the function is so slow, optionally, you can also just load it from my saved file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95dc898299715690",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T21:49:48.208132Z",
     "start_time": "2025-05-06T21:49:48.184944Z"
    }
   },
   "outputs": [],
   "source": [
    "train_output = pd.read_csv(\"train_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a9ad901d93b9ed",
   "metadata": {},
   "source": [
    "Lastly, I needed some way to compare the selected text that I extracted vs the selected text that\n",
    "was given in the training data. On my first attempt, I tried just checking for how many words in the\n",
    "actual testing data was in my phrase as well, but I quickly realized that metric inherently favored longer phrases.\n",
    "In my second attempt, I wrote the following function, which uses Jaccard similarity to score the overlap between my two phrases.\n",
    "I did edit my extract_top_phrases several times in attempt to get this score higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f4e6bedaacc54e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T21:49:48.861611Z",
     "start_time": "2025-05-06T21:49:48.209287Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average overlap of selected text: 0.5612093413182613\n"
     ]
    }
   ],
   "source": [
    "def compare_to_train():\n",
    "    scores = []\n",
    "    for (idx, trow), (_, orow) in zip(train_df.iterrows(), train_output.iterrows()):\n",
    "        trow_words = set(trow[\"selected_text\"].split())\n",
    "        orow_words = set(str(orow[\"selected_text\"]).split())\n",
    "        overlap_score = len(trow_words & orow_words) / len(trow_words | orow_words) if trow_words | orow_words else 0\n",
    "        scores.append(overlap_score)\n",
    "    print(f\"Average overlap of selected text: {sum(scores)/len(scores)}\")\n",
    "    \n",
    "compare_to_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a163ab013428a59",
   "metadata": {},
   "source": [
    "The score I got in the end was 0.56, which indicates that just over half of the text I selected is the same as the testing data.\n",
    "I decided that this was a fairly good score, since it also relies on me previously having labelled the data correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e39840fd526aa6",
   "metadata": {},
   "source": [
    "Lastly, I ran the same phrase extraction on my testing data and outputted that to my final submission file.\n",
    "This file is what I would submit to Kaggle as my results. This run of `extract_top_phrases` is much faster, only about 3500 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "149a023cd415f0ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T21:49:53.189248Z",
     "start_time": "2025-05-06T21:49:48.859180Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3533"
     ]
    }
   ],
   "source": [
    "test_output = extract_top_phrases(test_df, test_features, clf, model)\n",
    "test_output.to_csv(\"test_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f32574",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
